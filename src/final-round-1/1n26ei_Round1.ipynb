{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 533 ms, total: 1.55 s\n",
      "Wall time: 990 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = \"https://s3.console.aws.amazon.com/s3/buckets/robi-datathon-dataset-2022?region=ap-southeast-1&tab=objects\".format(region, bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-191805398547'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'robi-datathon-dataset-2022/data/transaction'\n",
    "data_key = 'part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://robi-datathon-dataset-2022/data/transaction/part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing all the *Retail Points* in Bangladesh**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropped all the points that were out side of the maximum rectangle that bounds Bangladesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_points = df_agent.drop(columns=['ts']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_points.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_points['agent'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "bd_dist = json.load(open(\"bd.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_geo(retail_points,\n",
    "    lat='lat',\n",
    "    lon='lon',\n",
    "    color='transactions',\n",
    "    geojson=bd_dist,\n",
    "    opacity=1,\n",
    "    size_max=5,\n",
    "    width=1080,\n",
    "    height=1920,\n",
    "    scope='asia',\n",
    "    hover_name='transactions'\n",
    ")\n",
    "fig.update_layout(title = 'Retail points', title_x=0.5)\n",
    "fig.update_geos(fitbounds='locations', visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is peak hour in terms of number of transactions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed**: group by hour each transaction dataset > created (key, value) pair where key is hour and value is the number of transaction > Aggregated all the counts based on the key for all transactions > Got the hour (key) with maximum aggregated count of transactions\n",
    "\n",
    "**Assumption**: Each slice has consecutive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00008-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00008-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00009-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00009-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00010-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00010-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00011-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00011-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00012-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00012-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00013-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00013-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00014-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00014-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00015-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00015-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00016-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00016-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00017-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00017-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00018-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00018-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00019-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00019-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n"
     ]
    }
   ],
   "source": [
    "map_hour = [0.0]*24\n",
    "\n",
    "for i in range(0, 10):\n",
    "    data_key = f'part-0000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    print(f'Reading {data_key}')\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(f'Read {data_key}')\n",
    "    df = df.dropna()\n",
    "    print(f'Dropped Nones')\n",
    "    df['ts'] = pd.to_datetime(df['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "    print(f'Converted timeseries')\n",
    "    temp_2 = df.groupby(df[\"ts\"].dt.hour).count()['agent'].values\n",
    "    print(f'Group by')\n",
    "    lists_of_lists = [map_hour, temp_2]\n",
    "    temp = [sum(x) for x in zip(*lists_of_lists)]\n",
    "\n",
    "for i in range(10, 20):\n",
    "    data_key = f'part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    print(f'Reading {data_key}')\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(f'Read {data_key}')\n",
    "    df = df.dropna()\n",
    "    print(f'Dropped Nones')\n",
    "    df['ts'] = pd.to_datetime(df['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "    print(f'Converted timeseries')\n",
    "    temp_2 = df.groupby(df[\"ts\"].dt.hour).count()['agent'].values\n",
    "    print(f'Group by')\n",
    "    lists_of_lists = [map_hour, temp_2]\n",
    "    temp = [sum(x) for x in zip(*lists_of_lists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620407.0\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "max_value = max(temp)\n",
    "print(max_value)\n",
    "max_index = temp.index(max_value)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is peak hour in terms of sales?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed**: group by hour each transaction dataset > created (key, value) pair where key is hour and value is the aggregated value of the transactions > Aggregated all the counts based on the key for all transactions > Got the hour (key) with maximum aggregated sum of price.\n",
    "\n",
    "**Assumption**: Each slice has consecutive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading s3://robi-datathon-dataset-2022/data/product.csv\n",
      "Read s3://robi-datathon-dataset-2022/data/product.csv\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://robi-datathon-dataset-2022/data/product.csv'\n",
    "\n",
    "print(f'Reading {data_location}')\n",
    "df_prod = pd.read_csv(data_location)\n",
    "print(f'Read {data_location}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>dt</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.00</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Product1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.00</td>\n",
       "      <td>20220310</td>\n",
       "      <td>Product2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.00</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Product3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.00</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Product4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.02</td>\n",
       "      <td>20220305</td>\n",
       "      <td>Product5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price        dt   product\n",
       "0   4.00  20220101  Product1\n",
       "1   7.00  20220310  Product2\n",
       "2   8.00  20220101  Product3\n",
       "3   9.00  20220101  Product4\n",
       "4   9.02  20220305  Product5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00008-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00008-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00009-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00009-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00010-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00010-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00011-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00011-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00012-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00012-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00013-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00013-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00014-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00014-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00015-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00015-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00016-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00016-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00017-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00017-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00018-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00018-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n",
      "Reading part-00019-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Read part-00019-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Dropped Nones\n",
      "Converted timeseries\n",
      "Group by\n"
     ]
    }
   ],
   "source": [
    "map_hour = [0.0]*24\n",
    "\n",
    "for i in range(0, 10):\n",
    "    data_key = f'part-0000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    print(f'Reading {data_key}')\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(f'Read {data_key}')\n",
    "    df = df.dropna()\n",
    "    print(f'Dropped Nones')\n",
    "    df = pd.merge(df, df_prod, on=['product'])\n",
    "    df['ts'] = pd.to_datetime(df['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "    print(f'Converted timeseries')\n",
    "    temp_2 = df.groupby(df[\"ts\"].dt.hour).sum()['price']\n",
    "    print(f'Group by')\n",
    "    lists_of_lists = [map_hour, temp_2]\n",
    "    temp = [sum(x) for x in zip(*lists_of_lists)]\n",
    "\n",
    "for i in range(10, 20):\n",
    "    data_key = f'part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    print(f'Reading {data_key}')\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(f'Read {data_key}')\n",
    "    df = df.dropna()\n",
    "    print(f'Dropped Nones')\n",
    "    df = pd.merge(df, df_prod, on=['product'])\n",
    "    df['ts'] = pd.to_datetime(df['ts'], format='%Y-%m-%d %H:%M:%S')\n",
    "    print(f'Converted timeseries')\n",
    "    temp_2 = df.groupby(df[\"ts\"].dt.hour).sum()['price']\n",
    "    print(f'Group by')\n",
    "    lists_of_lists = [map_hour, temp_2]\n",
    "    temp = [sum(x) for x in zip(*lists_of_lists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31492866.03\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "max_value = max(temp)\n",
    "print(max_value)\n",
    "max_index = temp.index(max_value)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Which customer was travelling the most?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed**: group by customer > sorted by timestamp > distance between consecutive samples > Sum-up all distance for each customer > save list of distance for customers(key) for each slice > merge the lists for all slices by summing up against keys > get highest distance.\n",
    "\n",
    "**Assumption**: Each slice has consecutive samples\n",
    "\n",
    "*Processes are done using multiprocessing nad numpy arrays for faster execution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading s3://robi-datathon-dataset-2022/data/agent.csv\n",
      "Read s3://robi-datathon-dataset-2022/data/agent.csv\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://robi-datathon-dataset-2022/data/agent.csv'\n",
    "\n",
    "print(f'Reading {data_location}')\n",
    "df_agent = pd.read_csv(data_location)\n",
    "print(f'Read {data_location}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>632791e6ac867e8257e6fccb2f5ef5eb8e6eed8ee2bd03...</td>\n",
       "      <td>23.541481</td>\n",
       "      <td>89.170306</td>\n",
       "      <td>2022-06-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c48ff60fa46230c0ebbcfa1e7064224bda84952554f6ff...</td>\n",
       "      <td>23.486257</td>\n",
       "      <td>89.251074</td>\n",
       "      <td>2022-06-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264c4537ccffb2b544ed7f4bac7d8cf04f08ccb23ea634...</td>\n",
       "      <td>23.563920</td>\n",
       "      <td>89.047684</td>\n",
       "      <td>2022-06-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7a876f47bde543409b9e546ab128bcb10fe304dbd9cecb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a226d44dec4bae00991d8e8d9ae370a77640e7e62734f...</td>\n",
       "      <td>23.553360</td>\n",
       "      <td>89.098775</td>\n",
       "      <td>2022-06-19 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               agent        lat        lon  \\\n",
       "0  632791e6ac867e8257e6fccb2f5ef5eb8e6eed8ee2bd03...  23.541481  89.170306   \n",
       "1  c48ff60fa46230c0ebbcfa1e7064224bda84952554f6ff...  23.486257  89.251074   \n",
       "2  264c4537ccffb2b544ed7f4bac7d8cf04f08ccb23ea634...  23.563920  89.047684   \n",
       "3  7a876f47bde543409b9e546ab128bcb10fe304dbd9cecb...        NaN        NaN   \n",
       "4  5a226d44dec4bae00991d8e8d9ae370a77640e7e62734f...  23.553360  89.098775   \n",
       "\n",
       "                    ts  \n",
       "0  2022-06-19 00:00:00  \n",
       "1  2022-06-19 00:00:00  \n",
       "2  2022-06-19 00:00:00  \n",
       "3  2022-06-19 00:00:00  \n",
       "4  2022-06-19 00:00:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dist(df_x):\n",
    "    sum = 0\n",
    "    p1 = (0.0, 0.0)\n",
    "    idx = 0\n",
    "    for row in df_x.values:\n",
    "        if idx == 0:\n",
    "            # print(\"skipped\")\n",
    "            p1 = (row[5], row[6])\n",
    "            idx = idx + 1\n",
    "            continue\n",
    "        \n",
    "        p2 = (row[5], row[6])\n",
    "        sum += distance.great_circle(p1, p2).km\n",
    "        # print(f'Exec: {p1} {p2}: {sum}')\n",
    "        p1 = p2\n",
    "        idx = idx + 1\n",
    "    return(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://robi-datathon-dataset-2022/data/transaction/part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00008-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00009-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00010-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00011-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00012-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00013-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00014-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00015-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00016-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00017-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00018-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv',\n",
       " 's3://robi-datathon-dataset-2022/data/transaction/part-00019-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list = []\n",
    "for i in range(0, 10):\n",
    "    data_key = f'part-0000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    param_list.append(data_location)\n",
    "    \n",
    "for i in range(10, 20):\n",
    "    data_key = f'part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    param_list.append(data_location)\n",
    "    \n",
    "param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slices(data_location):\n",
    "    print(f\"Reading {data_location}\")\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(f'Read')\n",
    "    df = df.dropna()\n",
    "    print(f'Dropped Nones')\n",
    "    df = pd.merge(df, df_agent, on=['agent'])\n",
    "    temp_2 = df.groupby(df[\"customer\"]).apply(sum_dist)\n",
    "    print(f'Group by')\n",
    "    return temp_2.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00002-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00004-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00003-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00006-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00007-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n",
      "Reading s3://robi-datathon-dataset-2022/data/transaction/part-00005-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = Pool(8) # on 8 processors\n",
    "    data_outputs = pool.map(process_slices, param_list)\n",
    "finally: # To make sure processes are closed in the end, even if errors happen\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "for d in data_outputs:\n",
    "    c.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dict(c)\n",
    "Keymax = max(zip(c.values(), c.keys()))[1]\n",
    "print(Keymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Draft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_squares.get(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cubes.get(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    data_key = f'part-0000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    df = pd.concat([df,pd.read_csv(data_location)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    data_key = f'part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv'\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "    df = pd.concat([df,pd.read_csv(data_location)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98266017, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 98266017 entries, 0 to 4913306\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   agent     object\n",
      " 1   customer  object\n",
      " 2   product   object\n",
      " 3   dt        int64 \n",
      " 4   ts        object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.4+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agent = pd.read_csv('s3://{}/{}'.format(bucket, data_key))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
